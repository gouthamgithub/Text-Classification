{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from xml.dom import minidom\n",
    "import xml.etree.ElementTree as et\n",
    "import numpy as np\n",
    "\n",
    "df_rows = []\n",
    "df_columns = [\"itemid\", \"headline\",\"text\", \"bip:topics\", \"dc.date.published\", \"XML filename\"]\n",
    "files_path = (r'C:\\Users\\satya\\Downloads\\Goutham\\mlbd\\New folder\\New folder\\New folder')\n",
    "files = []\n",
    "column_bip_topics = []\n",
    "for r, d, f in os.walk(files_path):\n",
    "    for file in f:\n",
    "        if '.xml' in file:\n",
    "            files.append(os.path.join(r,file))\n",
    "#-----------------------Step 3--------------------------------------------------------------\n",
    "def GetDataFrame(files): ##Function to extract DataFrame \n",
    "    for name in files:\n",
    "        address = name.split('\\\\')\n",
    "        xml_filename = address[-1] ##GET FILE NAME\n",
    "        xml_bip_topics = ''\n",
    "        xml_text = ''\n",
    "        dom = et.parse(name)\n",
    "        root = dom.getroot()\n",
    "        xml_itemId = root.attrib.get(\"itemid\") ##GET ITEM ID\n",
    "        xml_heading = root.find(\"headline\").text ##GET HEADLINE\n",
    "        for child in root:\n",
    "                if(child.tag == 'text'):\n",
    "                    for textnode in child:\n",
    "                        if(textnode.tag == 'p'):\n",
    "                            #print(dir(textnode))\n",
    "                            xml_text = xml_text + \" \" + textnode.text##GET TEXT\n",
    "                for j in child.iter('metadata'):\n",
    "                    for bip in j.iterfind(\".//codes[@class='bip:topics:1.0']\"):\n",
    "                            for bipnodes in bip.iter(\"code\"):\n",
    "                                xml_bip_topics = bipnodes.attrib.get(\"code\") + \",\" + xml_bip_topics ##GET BIP_CODES\n",
    "                    for date in j.iterfind(\".//dc[@element='dc.date.published']\"):\n",
    "                                   xml_date=date.get('value') ##GET DATE\n",
    "        column_bip_topics.append(xml_bip_topics)\n",
    "        df_rows.append({\"headline\" : xml_heading, \"text\" : xml_text, \"bip:topics\" : xml_bip_topics, \"dc.date.published\" : xml_date,  \"itemid\" : xml_itemId,  \"XML filename\" : xml_filename })\n",
    "    Final_DF = pd.DataFrame(df_rows, columns = df_columns)\n",
    "    return Final_DF\n",
    "Final_DF_Values = GetDataFrame(files)\n",
    "Text_Column = (Final_DF_Values.loc[:,['text']]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Step 4--------------------------------------------------------------\n",
    "def GetAllBipTopics(column_bip_topics):\n",
    "    all_bips = []\n",
    "    first_bip = []\n",
    "    for b in column_bip_topics:\n",
    "        indiv_bips = str(b).split(\",\")\n",
    "        for bip in indiv_bips:\n",
    "            all_bips.append(bip)\n",
    "        first_bip.append(indiv_bips[0])\n",
    "    All_Indivitual_bips = set(all_bips)\n",
    "    All_Indivitual_bips = list(filter(None,All_Indivitual_bips)) ##All possible bip values\n",
    "    return (All_Indivitual_bips,first_bip)\n",
    "All_Indivitual_bips,first_bip = GetAllBipTopics(column_bip_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48375\n",
      "48375\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import sys\n",
    "##-----------------------Step 5--------------------------------------------------------------\n",
    "def GetFiltedText(Text_Column):\n",
    "    WL = WordNetLemmatizer()\n",
    "    PS = PorterStemmer()\n",
    "    StopWords = set(stopwords.words('english'))\n",
    "    Filtered_WordList = []\n",
    "    for text in Text_Column:\n",
    "        FilterNumber1 = (re.sub('[^A-Za-z ]+', '', str(text))).lower() ## To lower and Selecting only alphabets\n",
    "        Tokens = word_tokenize(FilterNumber1)\n",
    "        FilterNumber2 = []\n",
    "        Filtered_Sentence = \"\"\n",
    "        for word in Tokens:\n",
    "            if word not in StopWords:\n",
    "                FilterNumber2.append(PS.stem(WL.lemmatize(word))) ##Stemming, lemmatizing and removing stop words\n",
    "        for i in FilterNumber2:\n",
    "            if(len(i) != 1 and len(i) != 2):\n",
    "                Filtered_Sentence = Filtered_Sentence + \" \" + i\n",
    "        Filtered_WordList.append(Filtered_Sentence)\n",
    "    return Filtered_WordList\n",
    "Filtered_WordList = GetFiltedText(Text_Column)\n",
    "print(len(Filtered_WordList))\n",
    "print(len(first_bip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "##-----------------------Step 6--------------------------------------------------------------\n",
    "def FeatureSelection(Filtered_WordList,first_bip):\n",
    "    model = TV(max_features = 2000) ##Extract more frequency tokens first\n",
    "    tfidf = model.fit_transform(Filtered_WordList)\n",
    "    Feature_Name = model.get_feature_names()\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=1500) ##Select features based on labels using SelectKBest and chi2\n",
    "    fit = bestfeatures.fit(tfidf.toarray(),first_bip)\n",
    "    final_features = fit.transform(tfidf.toarray())\n",
    "    return final_features\n",
    "final_features = FeatureSelection(Filtered_WordList, first_bip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------Step 7 & 9--------------------------------------------------------------\n",
    "from sklearn import model_selection\n",
    "def CrossValidation(model):\n",
    "    score = model_selection.cross_val_score(model,final_features, first_bip, scoring='f1_weighted',cv=3)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: \n",
    "Since we have a large data set, using k-fold cross validation with a small k value(3 to 5) is the best way to divide train and test sets. Doing normal train set-test set split on imbalanced class distribution data set may lead to uneven distribution where a complete n-labbeled cases may fall in test set. And since our data set size is high, using a large number for 'k' will be computationally expensive. Hence small k value. We can mitigate overfitting as well since every data row is used in train set more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7547668932598809\n"
     ]
    }
   ],
   "source": [
    "##-----------------------Step 8--------------------------------------------------------------\n",
    "def SGDClasifier():\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    model = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "    score = CrossValidation(model)\n",
    "    return score\n",
    "Score = SGDClasifier()\n",
    "print(Score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: \n",
    "The best evaluater to use when we have imbalanced class distribution is F1-score. Even though Accuracy is most widely used, this metric treats all classes equally, where as f1 score keeps a balance between recall and precision and gives a better measure of incorrectly classified inputs. Similarly AUC is the complete area under ROC curse and averages all thesholds, where as F1 score points to particular theshold which is more accurate for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7526137585060657\n"
     ]
    }
   ],
   "source": [
    "##-----------------------Step 10--------------------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.metrics import f1_score\n",
    "model = rf(n_estimators = 20)\n",
    "score = CrossValidation(model)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6863940652848731\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()\n",
    "score = CrossValidation(model)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8050344061999762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "score = CrossValidation(model)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8030854947208944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "model = mlp(hidden_layer_sizes = (100,300))\n",
    "score = CrossValidation(model)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8180552207189958\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model = svm.LinearSVC()\n",
    "score = CrossValidation(model)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning all the mentioned classifiers and looking at the respective F1-scores, Linear Support Vector Classifier gave the best results, hence SVM is the best classifier for this Text dataset. However, given high computational power, MLP classifier(Neural Network) can be trained by tuning the hyperparameters(number of hidden layers) for much better results. Since Linear regression is best suited for continuos output, logistic regression is a more suitable for classification problems. Random forest considers multiple bootstrapped sets and trains in an iteration instead of training full dataset at once like decision tree. Hence, Random forest gave better results than decision tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
