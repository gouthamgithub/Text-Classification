{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48375\n",
      "48375\n",
      "Automatically created module for IPython interactive environment\n",
      "For n_clusters = 4 The average silhouette_score is : 0.01136250615616302\n",
      "For n_clusters = 5 The average silhouette_score is : 0.013225579609507997\n",
      "For n_clusters = 6 The average silhouette_score is : 0.015675860712247067\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from xml.dom import minidom\n",
    "import xml.etree.ElementTree as et\n",
    "import numpy as np\n",
    "\n",
    "df_rows = []\n",
    "df_columns = [\"itemid\", \"text\", \"bip:topics\"]\n",
    "files_path = (r'C:\\Users\\gouth\\OneDrive\\Desktop\\Dalhousie\\ML BD\\Assignments\\Assignment 1\\Data\\New folder\\New folder')\n",
    "files = []\n",
    "column_bip_topics = []\n",
    "for r, d, f in os.walk(files_path):\n",
    "    for file in f:\n",
    "        if '.xml' in file:\n",
    "            files.append(os.path.join(r,file))\n",
    "#-----------------------Get Text and bip values from all files--------------------------------------------------------------\n",
    "def GetDataFrame(files): \n",
    "    for name in files:\n",
    "        address = name.split('\\\\')\n",
    "        \n",
    "        xml_bip_topics = ''\n",
    "        xml_text = ''\n",
    "        dom = et.parse(name)\n",
    "        root = dom.getroot()\n",
    "        xml_itemId = root.attrib.get(\"itemid\") ##GET ITEM ID\n",
    "        \n",
    "        for child in root:\n",
    "                if(child.tag == 'text'):\n",
    "                    for textnode in child:\n",
    "                        if(textnode.tag == 'p'):\n",
    "                            #print(dir(textnode))\n",
    "                            xml_text = xml_text + \" \" + textnode.text##GET TEXT\n",
    "                for j in child.iter('metadata'):\n",
    "                    for bip in j.iterfind(\".//codes[@class='bip:topics:1.0']\"):\n",
    "                            for bipnodes in bip.iter(\"code\"):\n",
    "                                xml_bip_topics = bipnodes.attrib.get(\"code\") + \",\" + xml_bip_topics ##GET BIP_CODES\n",
    "                    \n",
    "        column_bip_topics.append(xml_bip_topics)\n",
    "        df_rows.append({\"text\" : xml_text, \"bip:topics\" : xml_bip_topics, \"itemid\" : xml_itemId })\n",
    "    Final_DF = pd.DataFrame(df_rows, columns = df_columns)\n",
    "    return Final_DF\n",
    "Final_DF_Values = GetDataFrame(files)\n",
    "Text_Column = (Final_DF_Values.loc[:,['text']]).values\n",
    "#---------------------------------Extract unique Bip Values------------------------------------\n",
    "def GetAllBipTopics(column_bip_topics):\n",
    "    all_bips = []\n",
    "    first_bip = []\n",
    "    for b in column_bip_topics:\n",
    "        indiv_bips = str(b).split(\",\")\n",
    "        for bip in indiv_bips:\n",
    "            all_bips.append(bip)\n",
    "        first_bip.append(indiv_bips[0])\n",
    "    All_Indivitual_bips = set(all_bips)\n",
    "    All_Indivitual_bips = list(filter(None,All_Indivitual_bips)) ##All possible bip values\n",
    "    return (All_Indivitual_bips,first_bip)\n",
    "All_Indivitual_bips,first_bip = GetAllBipTopics(column_bip_topics)\n",
    "for i in range(len(first_bip)):\n",
    "    if len(first_bip[i]) == 0:\n",
    "        first_bip[i] = 'NA'\n",
    "#---------------------------------------Data Preprocessing Step------------------------------------------------\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import sys\n",
    "def GetFiltedText(Text_Column):\n",
    "    WL = WordNetLemmatizer()\n",
    "    PS = PorterStemmer()\n",
    "    StopWords = set(stopwords.words('english'))\n",
    "    Filtered_WordList = []\n",
    "    for text in Text_Column:\n",
    "        FilterNumber1 = (re.sub('[^A-Za-z ]+', '', str(text))).lower() ## To lower and Selecting only alphabets\n",
    "        Tokens = word_tokenize(FilterNumber1)\n",
    "        FilterNumber2 = []\n",
    "        Filtered_Sentence = \"\"\n",
    "        for word in Tokens:\n",
    "            if word not in StopWords:\n",
    "                FilterNumber2.append(PS.stem(WL.lemmatize(word))) ##Stemming, lemmatizing and removing stop words\n",
    "        for i in FilterNumber2:\n",
    "            if(len(i) != 1 and len(i) != 2):\n",
    "                Filtered_Sentence = Filtered_Sentence + \" \" + i\n",
    "        Filtered_WordList.append(Filtered_Sentence)\n",
    "    return Filtered_WordList\n",
    "Filtered_WordList = GetFiltedText(Text_Column)\n",
    "print(len(Filtered_WordList))\n",
    "print(len(first_bip))\n",
    "#----------------------------------------Using TF-IDF----------------------------------------------\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "def FeatureSelection(Filtered_WordList,first_bip):\n",
    "    model = TV(max_features = 1800) ##Extract more frequency tokens first\n",
    "    tfidf = model.fit_transform(Filtered_WordList)\n",
    "    final_features = tfidf.toarray()\n",
    "    return final_features\n",
    "final_features = FeatureSelection(Filtered_WordList, first_bip)\n",
    "#---------------------------------------Silhouette Score Calculation----------------------------------\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "print(__doc__)\n",
    "range_n_clusters = [4,5,6]\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(final_features)\n",
    "    silhouette_avg = silhouette_score(final_features, cluster_labels, metric='euclidean')\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "#-------------------------------------------Clustering dataset by Silhouette value----------------------\n",
    "One_big_matrix = []\n",
    "One_big_matrix = np.column_stack((cluster_labels,final_features.astype(np.object),first_bip))\n",
    "One_big_matrix = One_big_matrix[One_big_matrix[:,0].argsort()]\n",
    "#One_big_matrix[:,1:-1] = One_big_matrix[:,1:-1].astype(np.float64)\n",
    "a0 = np.count_nonzero(cluster_labels == 0)\n",
    "a1 = a0 + np.count_nonzero(cluster_labels == 1)\n",
    "a2 = a1 + np.count_nonzero(cluster_labels == 2)\n",
    "a3 = a2 + np.count_nonzero(cluster_labels == 3)\n",
    "a4 = a3 + np.count_nonzero(cluster_labels == 4)\n",
    "a5 = a4 + np.count_nonzero(cluster_labels == 5)\n",
    "First_Cluster = One_big_matrix[0:a0,:]\n",
    "Second_Cluster = One_big_matrix[a0:a1,:]\n",
    "Third_Cluster = One_big_matrix[a1:a2,:]\n",
    "Fourth_Cluster = One_big_matrix[a2:a3,:]\n",
    "Fifth_Cluster = One_big_matrix[a3:a4,:]\n",
    "Sixth_Cluster = One_big_matrix[a4:a5,:]\n",
    "#------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of First cluster:  0.8917311124330756\n",
      "Accuracy of Second cluster:  0.5595984943538268\n",
      "Accuracy of Third cluster:  0.8969404186795491\n",
      "Accuracy of Fourth cluster:  0.5617521800851755\n",
      "Accuracy of Fifth cluster:  0.8990719257540604\n",
      "Accuracy of Sixth cluster:  0.9794372294372294\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(First_Cluster[:,1:-1], First_Cluster[:,-1], test_size=0.3)\n",
    "mlp1 = mlp(hidden_layer_sizes=(30,30,30))\n",
    "mlp1.fit(X_train,y_train)\n",
    "y_pred = mlp1.predict(X_test)\n",
    "from sklearn.metrics import f1_score\n",
    "print('Accuracy of First cluster: ' ,accuracy_score(y_test, y_pred))\n",
    "#-------------------------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "X_train, X_test, y_train, y_test = train_test_split(Second_Cluster[:,1:-1], Second_Cluster[:,-1], test_size=0.3)\n",
    "RF = rf(n_estimators=100, max_depth=2, random_state=0)\n",
    "RF.fit(X_train, y_train)\n",
    "y_pred = RF.predict(X_test)\n",
    "print('Accuracy of Second cluster: ' ,accuracy_score(y_test, y_pred))\n",
    "#--------------------------------------------------------------------\n",
    "from sklearn import svm\n",
    "X_train, X_test, y_train, y_test = train_test_split(Third_Cluster[:,1:-1], Third_Cluster[:,-1], test_size=0.3)\n",
    "SVM = svm.LinearSVC()\n",
    "SVM.fit(X_train, y_train)\n",
    "y_pred = SVM.predict(X_test)\n",
    "print('Accuracy of Third cluster: ' ,accuracy_score(y_test, y_pred))\n",
    "#----------------------------------------------------------------------\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(Fourth_Cluster[:,1:-1], Fourth_Cluster[:,-1], test_size=0.3)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy of Fourth cluster: ' ,accuracy_score(y_test, y_pred))\n",
    "#------------------------------------------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(Fifth_Cluster[:,1:-1], Fifth_Cluster[:,-1], test_size=0.3)\n",
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print('Accuracy of Fifth cluster: ' ,accuracy_score(y_test, y_pred))\n",
    "#----------------------------------------------------------------------\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(Sixth_Cluster[:,1:-1], Sixth_Cluster[:,-1], test_size=0.3)\n",
    "clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy of Sixth cluster: ' ,accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "One_big_matrix1 = []\n",
    "One_big_matrix1 = np.vstack((cluster_labels,Filtered_WordList,first_bip))\n",
    "One_big_matrix1 = (One_big_matrix1[One_big_matrix1[:,0].argsort()]).T\n",
    "b0 = np.count_nonzero(cluster_labels == 0)\n",
    "b1 = b0 + np.count_nonzero(cluster_labels == 1)\n",
    "b2 = b1 + np.count_nonzero(cluster_labels == 2)\n",
    "b3 = b2 + np.count_nonzero(cluster_labels == 3)\n",
    "b4 = b3 + np.count_nonzero(cluster_labels == 4)\n",
    "b5 = b4 + np.count_nonzero(cluster_labels == 5)\n",
    "First_Cluster = One_big_matrix1[0:b0,:]\n",
    "Second_Cluster = One_big_matrix1[b0:b1,:]\n",
    "Third_Cluster = One_big_matrix1[b1:b2,:]\n",
    "Fourth_Cluster = One_big_matrix1[b2:b3,:]\n",
    "Fifth_Cluster = One_big_matrix1[b3:b4,:]\n",
    "Sixth_Cluster = One_big_matrix1[b4:b5,:]\n",
    "#-----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5601, 1500)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1500, 100)         700       \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 1500, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 47)                3055      \n",
      "=================================================================\n",
      "Total params: 45,995\n",
      "Trainable params: 45,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3920 samples, validate on 1681 samples\n",
      "Epoch 1/3\n",
      " - 162s - loss: 0.1029 - acc: 0.9787 - val_loss: 0.1028 - val_acc: 0.9787\n",
      "Epoch 2/3\n",
      " - 156s - loss: 0.1029 - acc: 0.9787 - val_loss: 0.1027 - val_acc: 0.9787\n",
      "Epoch 3/3\n",
      " - 164s - loss: 0.1028 - acc: 0.9787 - val_loss: 0.1027 - val_acc: 0.9787\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------ FIRST CLUSTER---------------------------------\n",
    "import gensim \n",
    "EMBEDDING_DIM = 100\n",
    "model = gensim.models.Word2Vec(sentences=First_Cluster[:,1], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "fn = 'word2vec.txt'\n",
    "model.wv.save_word2vec_format(fn, binary=False)\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#------------------------------------------------------------------------------\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(First_Cluster[:,1])\n",
    "sequences = tokenizer_obj.texts_to_sequences(First_Cluster[:,1])\n",
    "word_index = tokenizer_obj.word_index\n",
    "After_padding = pad_sequences(sequences, maxlen=1500)\n",
    "print(After_padding.shape)\n",
    "#-----------------------------------------------------------------------------------\n",
    "tokenizer_obj11 = Tokenizer()\n",
    "tokenizer_obj11.fit_on_texts(First_Cluster[:,2])\n",
    "sequences1 = tokenizer_obj11.texts_to_sequences(First_Cluster[:,2])\n",
    "sequences11 = []\n",
    "#print(int(sequences1[0]))\n",
    "for i in sequences1:\n",
    "    for j in i:\n",
    "        sequences11.append(j)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "sequences11 = to_categorical(sequences11)\n",
    "Network(After_padding, sequences11, word_index,embeddings_index) ##calling Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 1500, 100)         700       \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_19 (Spatia (None, 1500, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 56)                3640      \n",
      "=================================================================\n",
      "Total params: 46,580\n",
      "Trainable params: 46,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5577 samples, validate on 2391 samples\n",
      "Epoch 1/3\n",
      " - 130s - loss: 0.0896 - acc: 0.9821 - val_loss: 0.0893 - val_acc: 0.9821\n",
      "Epoch 2/3\n",
      " - 129s - loss: 0.0895 - acc: 0.9821 - val_loss: 0.0893 - val_acc: 0.9821\n",
      "Epoch 3/3\n",
      " - 135s - loss: 0.0894 - acc: 0.9821 - val_loss: 0.0892 - val_acc: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d0f5c83c88>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------Second Cluster----------------------------------------\n",
    "model = gensim.models.Word2Vec(sentences=Second_Cluster[:,1], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "fn = 'word2vec.txt'\n",
    "model.wv.save_word2vec_format(fn, binary=False)\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(Second_Cluster[:,1])\n",
    "sequences = tokenizer_obj.texts_to_sequences(Second_Cluster[:,1])\n",
    "word_index = tokenizer_obj.word_index\n",
    "After_padding = pad_sequences(sequences, maxlen=1500)\n",
    "tokenizer_obj11 = Tokenizer()\n",
    "tokenizer_obj11.fit_on_texts(Second_Cluster[:,2])\n",
    "sequences1 = tokenizer_obj11.texts_to_sequences(Second_Cluster[:,2])\n",
    "sequences11 = []\n",
    "for i in sequences1:\n",
    "    for j in i:\n",
    "        sequences11.append(j)\n",
    "Network(After_padding, sequences11, word_index,embeddings_index) ##calling Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 1500, 100)         700       \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_21 (Spatia (None, 1500, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                4160      \n",
      "=================================================================\n",
      "Total params: 47,100\n",
      "Trainable params: 47,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8692 samples, validate on 3726 samples\n",
      "Epoch 1/3\n",
      " - 231s - loss: 0.0807 - acc: 0.9844 - val_loss: 0.0806 - val_acc: 0.9844\n",
      "Epoch 2/3\n",
      " - 245s - loss: 0.0807 - acc: 0.9844 - val_loss: 0.0806 - val_acc: 0.9844\n",
      "Epoch 3/3\n",
      " - 262s - loss: 0.0806 - acc: 0.9844 - val_loss: 0.0805 - val_acc: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d0fec6b8c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------Third Cluster----------------------------------------\n",
    "model = gensim.models.Word2Vec(sentences=Third_Cluster[:,1], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "fn = 'word2vec.txt'\n",
    "model.wv.save_word2vec_format(fn, binary=False)\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(Third_Cluster[:,1])\n",
    "sequences = tokenizer_obj.texts_to_sequences(Third_Cluster[:,1])\n",
    "word_index = tokenizer_obj.word_index\n",
    "After_padding = pad_sequences(sequences, maxlen=1500)\n",
    "#-----------------------------------------------------------------------------------\n",
    "tokenizer_obj11 = Tokenizer()\n",
    "tokenizer_obj11.fit_on_texts(Third_Cluster[:,2])\n",
    "sequences1 = tokenizer_obj11.texts_to_sequences(Third_Cluster[:,2])\n",
    "sequences11 = []\n",
    "for i in sequences1:\n",
    "    for j in i:\n",
    "        sequences11.append(j)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "sequences11 = to_categorical(sequences11)\n",
    "EMBEDDING_DIM =100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "Network(After_padding, sequences11, word_index,embeddings_index)#Calling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 1500, 100)         700       \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_23 (Spatia (None, 1500, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 66)                4290      \n",
      "=================================================================\n",
      "Total params: 47,230\n",
      "Trainable params: 47,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 11505 samples, validate on 4931 samples\n",
      "Epoch 1/3\n",
      " - 404s - loss: 0.0784 - acc: 0.9848 - val_loss: 0.0784 - val_acc: 0.9848\n",
      "Epoch 2/3\n",
      " - 485s - loss: 0.0784 - acc: 0.9848 - val_loss: 0.0783 - val_acc: 0.9848\n",
      "Epoch 3/3\n",
      " - 571s - loss: 0.0783 - acc: 0.9848 - val_loss: 0.0782 - val_acc: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d0f37b72c8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------Fourth Cluster----------------------------------------\n",
    "model = gensim.models.Word2Vec(sentences=Fourth_Cluster[:,1], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "fn = 'word2vec.txt'\n",
    "model.wv.save_word2vec_format(fn, binary=False)\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(Fourth_Cluster[:,1])\n",
    "sequences = tokenizer_obj.texts_to_sequences(Fourth_Cluster[:,1])\n",
    "word_index = tokenizer_obj.word_index\n",
    "After_padding = pad_sequences(sequences, maxlen=1500)\n",
    "tokenizer_obj11 = Tokenizer()\n",
    "tokenizer_obj11.fit_on_texts(Fourth_Cluster[:,2])\n",
    "sequences1 = tokenizer_obj11.texts_to_sequences(Fourth_Cluster[:,2])\n",
    "sequences11 = []\n",
    "\n",
    "for i in sequences1:\n",
    "    for j in i:\n",
    "        sequences11.append(j)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "sequences11 = to_categorical(sequences11)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "sequences11 = to_categorical(sequences11)\n",
    "EMBEDDING_DIM =100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "Network(After_padding, sequences11, word_index,embeddings_index)#Calling neural network\n",
    "#------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 1500, 100)         700       \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_25 (Spatia (None, 1500, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 48)                3120      \n",
      "=================================================================\n",
      "Total params: 46,060\n",
      "Trainable params: 46,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2010 samples, validate on 862 samples\n",
      "Epoch 1/3\n",
      " - 120s - loss: 0.1016 - acc: 0.9792 - val_loss: 0.1015 - val_acc: 0.9792\n",
      "Epoch 2/3\n",
      " - 111s - loss: 0.1016 - acc: 0.9792 - val_loss: 0.1015 - val_acc: 0.9792\n",
      "Epoch 3/3\n",
      " - 114s - loss: 0.1015 - acc: 0.9792 - val_loss: 0.1014 - val_acc: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d0f4479708>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------Fifth Cluster----------------------------------------\n",
    "model = gensim.models.Word2Vec(sentences=Fifth_Cluster[:,1], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "fn = 'word2vec.txt'\n",
    "model.wv.save_word2vec_format(fn, binary=False)\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(Fifth_Cluster[:,1])\n",
    "sequences = tokenizer_obj.texts_to_sequences(Fifth_Cluster[:,1])\n",
    "word_index = tokenizer_obj.word_index\n",
    "After_padding = pad_sequences(sequences, maxlen=1500)\n",
    "#-----------------------------------------------------------------------------------\n",
    "tokenizer_obj11 = Tokenizer()\n",
    "tokenizer_obj11.fit_on_texts(Fifth_Cluster[:,2])\n",
    "sequences1 = tokenizer_obj11.texts_to_sequences(Fifth_Cluster[:,2])\n",
    "sequences11 = []\n",
    "\n",
    "for i in sequences1:\n",
    "    for j in i:\n",
    "        sequences11.append(j)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "sequences11 = to_categorical(sequences11)\n",
    "\n",
    "EMBEDDING_DIM =100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "Network(After_padding, sequences11, word_index,embeddings_index)#Calling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 1500, 100)         700       \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_27 (Spatia (None, 1500, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 45,475\n",
      "Trainable params: 45,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2156 samples, validate on 924 samples\n",
      "Epoch 1/3\n",
      " - 138s - loss: 0.1196 - acc: 0.9744 - val_loss: 0.1194 - val_acc: 0.9744\n",
      "Epoch 2/3\n",
      " - 127s - loss: 0.1195 - acc: 0.9744 - val_loss: 0.1194 - val_acc: 0.9744\n",
      "Epoch 3/3\n",
      " - 137s - loss: 0.1196 - acc: 0.9744 - val_loss: 0.1193 - val_acc: 0.9744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d0f42c3f08>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------Sixth Cluster------------------------------\n",
    "model = gensim.models.Word2Vec(sentences=Sixth_Cluster[:,1], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "fn = 'word2vec.txt'\n",
    "model.wv.save_word2vec_format(fn, binary=False)\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(Sixth_Cluster[:,1])\n",
    "sequences = tokenizer_obj.texts_to_sequences(Sixth_Cluster[:,1])\n",
    "word_index = tokenizer_obj.word_index\n",
    "After_padding = pad_sequences(sequences, maxlen=1500)\n",
    "#-----------------------------------------------------------------------------------\n",
    "tokenizer_obj11 = Tokenizer()\n",
    "tokenizer_obj11.fit_on_texts(Sixth_Cluster[:,2])\n",
    "sequences1 = tokenizer_obj11.texts_to_sequences(Sixth_Cluster[:,2])\n",
    "sequences11 = []\n",
    "for i in sequences1:\n",
    "    for j in i:\n",
    "        sequences11.append(j)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "sequences11 = to_categorical(sequences11)\n",
    "\n",
    "EMBEDDING_DIM =100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "Network(After_padding, sequences11, word_index,embeddings_index)#Calling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------CNN using LSTM function-----------------------------------------------\n",
    "def Network(ClusterInput, sequences11, word_index,embeddings_index, densesize):\n",
    "    EMBEDDING_DIM =100\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    num_words = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Embedding, Flatten\n",
    "    from keras.layers.convolutional import Conv1D\n",
    "    from keras.layers.convolutional import MaxPooling1D\n",
    "    from keras.initializers import Constant\n",
    "    from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "    from keras.optimizers import SGD\n",
    "    opt = SGD(lr=0.01)\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                input_length=1500,\n",
    "                                trainable=True)\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    model.add(SpatialDropout1D(0.8))\n",
    "    model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "    model.add(Dense(46, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    \n",
    "    X_train_pad, X_test_pad, y_train, y_test = train_test_split(ClusterInput, sequences11, test_size=0.3)\n",
    "    dense_num = np.size(y_train,1)\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    model.fit(X_train_pad, y_train, batch_size=128, epochs=3, validation_data=(X_test_pad, y_test), verbose=2)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
